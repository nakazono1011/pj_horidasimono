{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_log_error, r2_score\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import re, MeCab\n",
    "from glob import glob\n",
    "import mojimoji\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#環境変数,\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../auth/My First Project.json\"\n",
    "bucket_name = \"pj_horidasimono\"\n",
    "prefix=\"dataset/train/ElectricalAppliance\"\n",
    "#最大表示行数の指定（ここでは50行を指定）\n",
    "pd.set_option('display.max_rows', 200)\n",
    "#最大表示列数の指定（ここでは50列を指定）\n",
    "pd.set_option('display.max_columns', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def road_data_from_gcs(bucket_name, prefix):\n",
    "    client = storage.Client()\n",
    "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
    "    df = pd.DataFrame()\n",
    "    for blob in blobs:\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        r = storage.Blob(blob.name, bucket)\n",
    "        content = r.download_as_string()\n",
    "        df = df.append(pd.read_json(content))\n",
    "        print(f\"read file {blob.name}...\")\n",
    "\n",
    "    df = df.drop_duplicates(subset=\"url\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "def make_wakati(sentence):\n",
    "    # MeCabで分かち書き\n",
    "    sentence = tagger.parse(sentence)\n",
    "    sentence = mojimoji.zen_to_han(sentence)\n",
    "    # 半角全角英数字除去\n",
    "    #sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
    "    # 記号もろもろ除去\n",
    "    sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—♬◉ᴗ͈ˬ●★☆⭐️⭕⚡⚠o①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮♡⭐︎〇◎◆♦▼◇△□(：〜～＋=)／*&^%$#@!~`)♪ᴖ◡ᴖ{}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕＼〈〉？、､。｡・,\\./『』【】｢｣「」→←○《》≪≫\\n\\u3000⭕]+', \"\", sentence)\n",
    "    # 絵文字除去\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    sentence = emoji_pattern.sub(r'', sentence)\n",
    "    # スペースで区切って形態素の配列へ\n",
    "    #wakati = sentence.split(\" \")\n",
    "    # 空の要素は削除\n",
    "    #wakati = list(filter((\"\").__ne__, wakati))\n",
    "    return sentence\n",
    "\n",
    "def title_torkenize(sentence):\n",
    "    sentence = mojimoji.zen_to_han(sentence)\n",
    "    sentence = re.sub(\"[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒♬◉ᴗ͈ˬ—●★☆⭐️⭕⚡⚠o①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮♡⭐︎〇◎◆♦▼◇△□(：〜～＋=)／*&^%$#@!~`)♪ᴖ◡ᴖ{}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕＼〈〉？、､。｡・,\\./『』【】｢｣「」→←○《》≪≫\\n\\u3000]\", \" \", sentence)\n",
    "    sentence = re.sub(\"[あ-ん]\", \" \", sentence)\n",
    "    sentence = re.sub(\"( |　)+\", \" \", sentence)\n",
    "    sentence = sentence.lower()\n",
    "    #〇〇様専用を除く\n",
    "    sentence = re.sub(\"[^ ]*専用\", \"\", sentence)\n",
    "    sentence = re.sub(\"[^ ]*様\", \"\", sentence)\n",
    "    #1文字のアルファベットを除く\n",
    "    sentence = re.sub(\" [a-z]{1}[^(a-z)]\", \" \", sentence)\n",
    "    # 絵文字除去\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    sentence = emoji_pattern.sub(r'', sentence)\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess(df):\n",
    "    df[\"price\"] = df[\"price\"].str.replace(r\"\\D\", \"\").astype(np.float)\n",
    "    \n",
    "    #列ズレを修正\n",
    "    pattern = re.compile(r\"^(?!.*(傷や汚れあり|全体的に状態が悪い|やや傷や汚れあり|未使用に近い|目立った傷や汚れなし|新品、未使用)).+$\")\n",
    "    invalid = df[\"status\"].str.match(pattern)\n",
    "\n",
    "    df.loc[invalid, \"description\"] = df.loc[invalid, \"description\"] + \"\\n\" + df.loc[invalid, \"status\"]\n",
    "    df.loc[invalid, \"status\"]      = df.loc[invalid, \"shipping\"]\n",
    "    df.loc[invalid, \"shipping\"]    = df.loc[invalid, \"method\"]\n",
    "    df.loc[invalid, \"method\"]      = df.loc[invalid, \"region\"]\n",
    "    df.loc[invalid, \"period\"]      = \"未定\"\n",
    "    \n",
    "    df[\"title\"] = df[\"title\"] + \" \" + df[\"sub_category_1\"] + \" \" + df[\"sub_category_2\"] + \" \" + df[\"brand\"]\n",
    "    #df[\"text\"]  = df[\"title\"] + \" \" + df[\"description\"]\n",
    "\n",
    "    df = df.drop(columns=[\"sub_category_1\", \"sub_category_2\", \"brand\"])\n",
    "    \n",
    "    status_dict = {'新品、未使用': \"best\",\n",
    "                   '未使用に近い': \"Very Good\",\n",
    "                   '目立った傷や汚れなし': \"good\",\n",
    "                   '傷や汚れあり': \"Poor\",\n",
    "                   'やや傷や汚れあり': \"very poor\",\n",
    "                   '全体的に状態が悪い': \"worst\"\n",
    "                  }\n",
    "    \n",
    "    #配送負担をラベルエンコーディング\n",
    "    shipping_dict = {'送料込み(出品者負担)': 0, '着払い(購入者負担)': 1}\n",
    "\n",
    "    df[\"status\"] = df[\"status\"].map(status_dict)\n",
    "    df[\"shipping\"] = df[\"shipping\"].map(shipping_dict)\n",
    "    \n",
    "    #トークナイズ\n",
    "    df[\"title\"] = df[\"title\"].apply(title_torkenize)\n",
    "    df[\"description\"] = df[\"description\"].apply(make_wakati)\n",
    "    \n",
    "    #不要列削除\n",
    "    df = df.drop(columns=[\"url\", \"seller\", \"rating\", \"method\", \"region\", \"period\", \"recent_comment\", \"timestamp\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_ridge import ModelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file dataset/train/ElectricalAppliance20200921194130.json...\n",
      "read file dataset/train/ElectricalAppliance20200921212935.json...\n",
      "read file dataset/train/ElectricalAppliance20200921230505.json...\n",
      "read file dataset/train/ElectricalAppliance20200922003032.json...\n",
      "read file dataset/train/ElectricalAppliance20200922122051.json...\n",
      "read file dataset/train/ElectricalAppliance20200922195104.json...\n",
      "read file dataset/train/ElectricalAppliance20200923003024.json...\n",
      "read file dataset/train/ElectricalAppliance20200923005724.json...\n",
      "read file dataset/train/ElectricalAppliance20200923024628.json...\n",
      "read file dataset/train/ElectricalAppliance20200923194105.json...\n",
      "read file dataset/train/ElectricalAppliance20200923210407.json...\n",
      "read file dataset/train/ElectricalAppliance20200923224903.json...\n",
      "read file dataset/train/ElectricalAppliance20200924003019.json...\n",
      "read file dataset/train/ElectricalAppliance20200924093732.json...\n",
      "read file dataset/train/ElectricalAppliance20200924161706.json...\n",
      "read file dataset/train/ElectricalAppliance20200925003026.json...\n",
      "read file dataset/train/ElectricalAppliance20200926003123.json...\n",
      "read file dataset/train/ElectricalAppliance20200927003030.json...\n",
      "read file dataset/train/ElectricalAppliance20200928003019.json...\n",
      "read file dataset/train/ElectricalAppliance20200929003026.json...\n",
      "read file dataset/train/ElectricalAppliance20200930003029.json...\n",
      "read file dataset/train/ElectricalAppliance20201001003054.json...\n",
      "read file dataset/train/ElectricalAppliance20201002003031.json...\n",
      "read file dataset/train/ElectricalAppliance20201003003047.json...\n",
      "read file dataset/train/ElectricalAppliance20201004003034.json...\n",
      "read file dataset/train/ElectricalAppliance20201005003034.json...\n",
      "read file dataset/train/ElectricalAppliance20201006003040.json...\n",
      "read file dataset/train/ElectricalAppliance20201007003113.json...\n",
      "read file dataset/train/ElectricalAppliance20201008003124.json...\n",
      "read file dataset/train/ElectricalAppliance20201009003045.json...\n",
      "read file dataset/train/ElectricalAppliance20201010003046.json...\n",
      "read file dataset/train/ElectricalAppliance20201011003047.json...\n",
      "read file dataset/train/ElectricalAppliance20201012003036.json...\n",
      "read file dataset/train/ElectricalAppliance20201013003031.json...\n",
      "read file dataset/train/ElectricalAppliance20201014003059.json...\n",
      "read file dataset/train/ElectricalAppliance20201015003109.json...\n",
      "read file dataset/train/ElectricalAppliance20201016003051.json...\n",
      "read file dataset/train/ElectricalAppliance20201017003046.json...\n",
      "read file dataset/train/ElectricalAppliance20201018003121.json...\n",
      "read file dataset/train/ElectricalAppliance20201019003026.json...\n",
      "read file dataset/train/ElectricalAppliance20201020003040.json...\n",
      "read file dataset/train/ElectricalAppliance20201021003057.json...\n"
     ]
    }
   ],
   "source": [
    "df_ = road_data_from_gcs(bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess] done in 67 s\n",
      "[training] done in 113 s\n",
      "mape: 37.187%\n",
      "0.46028625975647264\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "def on_field(f: str, *vec):\n",
    "    return make_pipeline(FunctionTransformer(itemgetter(f), validate=False), *vec)\n",
    "\n",
    "def to_records(df: pd.DataFrame):\n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "vectorizer = make_union(\n",
    "            on_field(\"title\", Tfidf(max_features=100000, token_pattern=\"\\w+\")),\n",
    "            on_field(\"description\", Tfidf(max_features=100000, token_pattern=\"\\w+\", ngram_range=(1, 2))),\n",
    "            on_field(['shipping', 'status'],\n",
    "                 FunctionTransformer(to_records, validate=False), DictVectorizer()))\n",
    "\n",
    "with timer(\"preprocess\"):\n",
    "  df = preprocess(df_.copy())\n",
    "  X = df.drop(columns=\"price\")\n",
    "  y = df[\"price\"]\n",
    "\n",
    "  X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "with timer(\"training\"):\n",
    "  model = ModelRidge(vectorizer)\n",
    "  model.fit(X_train, y_train)\n",
    "    \n",
    "va_pred = np.expm1(model.predict(X_valid))\n",
    "score = np.sqrt(mean_squared_log_error(y_valid, va_pred))\n",
    "mape = np.mean(np.abs((y_valid - va_pred) / y_valid)) * 100\n",
    "print(\"mape: {:.3f}%\".format(mape))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df[\"price\"] = df[\"price\"].str.replace(r\"\\D\", \"\").astype(np.float)\n",
    "    \n",
    "    #列ズレを修正\n",
    "    pattern = re.compile(r\"^(?!.*(傷や汚れあり|全体的に状態が悪い|やや傷や汚れあり|未使用に近い|目立った傷や汚れなし|新品、未使用)).+$\")\n",
    "    invalid = df[\"status\"].str.match(pattern)\n",
    "\n",
    "    df.loc[invalid, \"description\"] = df.loc[invalid, \"description\"] + \"\\n\" + df.loc[invalid, \"status\"]\n",
    "    df.loc[invalid, \"status\"]      = df.loc[invalid, \"shipping\"]\n",
    "    df.loc[invalid, \"shipping\"]    = df.loc[invalid, \"method\"]\n",
    "    df.loc[invalid, \"method\"]      = df.loc[invalid, \"region\"]\n",
    "    df.loc[invalid, \"period\"]      = \"未定\"\n",
    "    \n",
    "    df[\"title\"] = df[\"title\"] + \" \" + df[\"sub_category_1\"] + \" \" + df[\"sub_category_2\"] + \" \" + df[\"brand\"]\n",
    "    df[\"text\"]  = df[\"title\"] + \" \" + df[\"description\"]\n",
    "\n",
    "    df = df.drop(columns=[\"sub_category_1\", \"sub_category_2\", \"brand\", \"description\"])\n",
    "    \n",
    "    status_dict = {'新品、未使用': \"best\",\n",
    "                   '未使用に近い': \"Very Good\",\n",
    "                   '目立った傷や汚れなし': \"good\",\n",
    "                   '傷や汚れあり': \"Poor\",\n",
    "                   'やや傷や汚れあり': \"very poor\",\n",
    "                   '全体的に状態が悪い': \"worst\"\n",
    "                  }\n",
    "    \n",
    "    #配送負担をラベルエンコーディング\n",
    "    shipping_dict = {'送料込み(出品者負担)': 0, '着払い(購入者負担)': 1}\n",
    "\n",
    "    df[\"status\"] = df[\"status\"].map(status_dict)\n",
    "    df[\"shipping\"] = df[\"shipping\"].map(shipping_dict)\n",
    "    \n",
    "    #不要列削除\n",
    "    df = df.drop(columns=[\"url\", \"seller\", \"rating\", \"method\", \"region\", \"period\", \"recent_comment\", \"timestamp\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df_.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>status</th>\n",
       "      <th>shipping</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ｼﾞｬﾝｸ iphone se silver 32 gb simﾌﾘｰ ｽﾏｰﾄﾌｫﾝ 携帯...</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>Poor</td>\n",
       "      <td>0</td>\n",
       "      <td>iPhone SE Silver 32 GB SIM ﾌﾘｰ 最近 まで 使用 し て い ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glidic tw 5000s ｵｰﾃﾞｨｵ機器 ｲﾔﾌｫﾝ</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>best</td>\n",
       "      <td>0</td>\n",
       "      <td>ﾜｲﾔﾚｽ ｲﾔﾎﾝ です ｡ 新品 未 使用 です ｡ 1 度 も 開封 せ ず ､ 自宅...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zoom sj ｵｰﾃﾞｨｵ機器 他</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>0</td>\n",
       "      <td>ﾊﾝﾃﾞｨ ﾀｲﾌﾟ の ﾚｺｰﾀﾞ です 風防 付き な の で 屋外 で の 録音 に ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>canon ﾌﾟﾘﾝﾀｲﾝｸ純正品6色ｾｯﾄ 350 351 標準容量 pc ﾀﾌﾞﾚｯﾄ ...</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>best</td>\n",
       "      <td>0</td>\n",
       "      <td>ﾌﾟﾘﾝﾀ 買い替え に 伴い 余剰 に なっ た の で 出品 し ます ｡ PGBK  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>panasonic np tcm4 2018年 生活家電 他</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>good</td>\n",
       "      <td>0</td>\n",
       "      <td>ご覧 いただき ありがとう ござい ます ｡ Panasonic 食洗 機 NP  TCM ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title    price     status  \\\n",
       "0  ｼﾞｬﾝｸ iphone se silver 32 gb simﾌﾘｰ ｽﾏｰﾄﾌｫﾝ 携帯...   3000.0       Poor   \n",
       "1                     glidic tw 5000s ｵｰﾃﾞｨｵ機器 ｲﾔﾌｫﾝ   4000.0       best   \n",
       "2                                 zoom sj ｵｰﾃﾞｨｵ機器 他   5000.0  Very Good   \n",
       "3  canon ﾌﾟﾘﾝﾀｲﾝｸ純正品6色ｾｯﾄ 350 351 標準容量 pc ﾀﾌﾞﾚｯﾄ ...   2500.0       best   \n",
       "4                     panasonic np tcm4 2018年 生活家電 他  21000.0       good   \n",
       "\n",
       "   shipping                                        description  \n",
       "0         0  iPhone SE Silver 32 GB SIM ﾌﾘｰ 最近 まで 使用 し て い ...  \n",
       "1         0  ﾜｲﾔﾚｽ ｲﾔﾎﾝ です ｡ 新品 未 使用 です ｡ 1 度 も 開封 せ ず ､ 自宅...  \n",
       "2         0  ﾊﾝﾃﾞｨ ﾀｲﾌﾟ の ﾚｺｰﾀﾞ です 風防 付き な の で 屋外 で の 録音 に ...  \n",
       "3         0  ﾌﾟﾘﾝﾀ 買い替え に 伴い 余剰 に なっ た の で 出品 し ます ｡ PGBK  ...  \n",
       "4         0  ご覧 いただき ありがとう ござい ます ｡ Panasonic 食洗 機 NP  TCM ...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_description = preprocess(df_.copy())[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iPhne SE Silver 32 GB SIM ﾌﾘｰ 最近 まで 使用 し て い た  iphneSE 第 1 世代 です  ﾀｯﾁ ﾊﾟﾈﾙ が たまに しか 反応 し ない ため  ｼﾞｬﾝｸ 品 と し て の 出品 です  本 品 は Sim ﾌﾘｰ 品 で  au  sftbank SIM で の 動作 は 確認 済み です  本体 以外 付属 品 は あり ませ ん  機種 名  iPhne SE ｶﾗｰ  Silver 容量  32 GB 購入 し た ｷｬﾘｱ  SIM ﾌﾘｰ 付属 品  なし \n",
      "1 ﾜｲﾔﾚｽ ｲﾔﾎﾝ です  新品 未 使用 です  1 度 も 開封 せ ず  自宅 保管 し おり まし た  使う 機会 が ない の で 出品 し ます  GLDIC ﾜｲﾔﾚｽ ｲﾔﾎﾝ Bluetth \n",
      "2 ﾊﾝﾃﾞｨ ﾀｲﾌﾟ の ﾚｺｰﾀﾞ です 風防 付き な の で 屋外 で の 録音 に 最適 です 数 回 しか 使っ て 無い の で ｷｽﾞ 等   全く あり ませ ん 動作 も 確認 済み です  問題 あり ませ ん 説明 書 は あり ませ ん が ﾈｯﾄ から DR 可 です \n",
      "3 ﾌﾟﾘﾝﾀ 買い替え に 伴い 余剰 に なっ た の で 出品 し ます  PGBK  BK  GY は ｾｯﾄ 品 を 購入 し て い た ため 先月 末 に 箱 は 開封 し て しまっ た の で  すぐ お 使い の 方 に お 譲り し たい と 思い ます  残り の 3 色 は  期限 まで いずれ も 18 か月 以上 あり ます   ｷﾔﾉﾝ  Cann \n",
      "4 ご覧 いただき ありがとう ござい ます  Panasnic 食洗 機 NP  TCM 4 知り合い に 譲り受け まし た が  私 は 使用 する 機会 が ない ため お 譲り いたし ます  2018 年 製造 品 に なり ます  使用 頻度 は 毎日 は 使わ ず 週 4 程度 です  通常 使用 に ともなう 軽微 な ｽﾚ ･ ｷｽﾞ あり ます よく 見 ない と 分から ない 程度 です  正面 向かっ て 左側 側面 に 小 傷 が あり ます が  小 傷 の ため あまり 気 に なり ませ ん  画像 10 枚 目 にて 確認 を お ねがい し ます  出品 に あたり  外せる 部品 の ところ まで お 掃除 ･ ｸﾘｰﾆﾝｸﾞ 済み です  全体 的 に とても 綺麗 です  臭い 等 は 若干 あり ます が あまり 使わ れ て い ない せい か 不快 な 臭い で は で は あり ませ ん   内容  本体  給水 ･ 排水 ﾎｰｽ  各 1 ･ ﾎｰｽ ﾊﾞﾝﾄﾞ ･ 吸盤 取扱 説明 書 欠品 し て おり ます が ﾒｰｶｰ WEB にて ご覧 に なれ ます  分岐 水栓 は 付属 し ませ ん  各 ご 家庭 の 蛇口 形状 に 合っ た 分岐 水栓 を ご 用意 し て 頂く 必要 が ござい ます  ご 家庭 に 合っ た 分岐 水栓 の お 買い求め 及び 取り付け 工事 は 購入 者 様 にて お 願い いたし ます   即 購入 可 です  ｺﾒﾝﾄ の 途中 で も 最初 に 購入 し た 方 が 優先 と なり ます   中古 品 の ため 神経 質 な 方 の ご 購入 は お 控え ください   画像 に 載せ て いる もの が お 渡し する 全て に なり ます   発送 は ﾌﾟﾁ ﾌﾟﾁ に 包み  ﾀﾞﾝ ﾎﾞｰﾙ に 入れ て 匿名 配送 らくらく ﾒﾙ ｶﾘ 便 にて 発送 いたし ます  ご 購入 お 待ち し て おり ます        下記 WEB より  洗浄 時間 約 29 分   ｽﾋﾟｰﾃﾞｨ ｺｰｽ   標準 ｺｰｽ 比 約 35  40 分 短縮    高温 洗浄  で 手洗い より も ｷﾚｲ ･ 節水  容量  食器 点数   18 点  ﾄﾞｱ ﾀｲﾌﾟ  前開き ﾄﾞｱ  乾燥 機能    運転 ｺｰｽ  標準 ｺｰｽ  ｽﾋﾟｰﾃﾞｨ ｺｰｽ  乾燥 のみ ｺｰｽ  本体 外形 寸法   は ﾄﾞｱ 開放 時  幅 470  奥行 300  598   高 さ 460  467  mm  製品 質量  約 12 kg  電源 ｺｰﾄﾞ の 長 さ  約 1  9 m  給 ･ 排水 ﾎｰｽ の 長 さ  給水 約 1  2 m  排水 約 1 m  庫内 容積  約 24 L  付属 品  専用 洗剤  吸盤  排水 ﾎｰｽ  給水 ﾎｰｽ  調整 脚  ﾊﾟﾅｿﾆｯｸ  Panasnic \n",
      "5 2013 年 ごろ から 使い 続け まし た が  CPU ﾌｧﾝ が ｶﾘｶﾘ 言い 出し た ため  2020  09 中旬 に 別 の PC を 買い換え まし た  知識 が ある 方 で あれ ば  再 利用 可能 か と 思い 出品 さ せ て 頂き まし た  なお  HDD は 再 利用 し て いる ため 本体 のみ と なり ます  送料 は ご 負担 頂き たく  低 価格 で 出品 いたし ます   仕様 や 状態 など ﾒｰｶｰ  hp 型番      Pavilin dv 6  A 7 B 44 AV  CPU      Intel Cre i 5  2450 M  2  5 GHz     ﾌｧﾝ が たまに ｶﾘｶﾘ と 音 を 立てる こと が あり ます  ﾒﾓﾘ    8 GB  4 GB  2     Elixir M 2 S 4 G 64 CB 8 HG 4 N  DI 2 枚 HDD      無し    2  5 ｲﾝﾁ の SATA が 使え ます 光学 ﾄﾞﾗｲﾌﾞ  DVD ﾏﾙﾁ ﾄﾞﾗｲﾌﾞ    若干  調子 が 悪い です  取り出し ﾎﾞﾀﾝ が 効か ない 時 が たまに あり ます  ｸﾞﾗﾌｨｯｸ  ｲﾝﾃﾙ HD ｸﾞﾗﾌｨｯｸｽ ﾃﾞｨｽﾌﾟﾚｲ  15  6 ｲﾝﾁ  1366 x 768  有線 LAN  Gigabit Lan 無線 LAN  IEEE 802  11 b  g  n OS  無し    Windws 10 の ﾌﾟﾛﾀﾞｸﾄ ｷｰ は 提供 でき ます      当 PC は Windws 7 から Windws 10 へ の 無償 ｱｯﾌﾟｸﾞﾚｰﾄﾞ 対象 でし た      winprductkey  exe で 抽出 し た Windws 10 の ﾌﾟﾛﾀﾞｸﾄ ｷｰ が あり ます       windws 10 無償 ｱｯﾌﾟｸﾞﾚｰﾄﾞ ｸﾘｰﾝ ｲﾝｽﾄｰﾙ winprductkey  exe  で     検索 し て ﾋｯﾄ する 手順 で 再 ｲﾝｽﾄｰﾙ できる か も しれ ませ ん      保証 は でき ませ ん      windws 7 の ﾌﾟﾛﾀﾞｸﾄ ｷｰ は 裏面 に 記載 さ れ て い ます  Office  なし ﾊﾞｯﾃﾘｰ  2019 年 に 交換 し まし た  \n",
      "6 宜しく どうぞ   付属 品 は 画像 が 全て です  問題 なく 動作 確認 済み です    \n",
      "7 audi − technica ATH  CK 3 TW BL 家電 量販 店 ﾜｲﾔﾚｽ ｲﾔﾎﾝ   人気 商品 上位 獲得 HEADPHONE TYPE  IN EAR  MICRO IN EAR 有 True Wireless  TRUE WIRELESS WIRED  NO WIRED WIRELESS clr  BLUE ｺｰﾄﾞﾚｽ 種類  増設 HP ﾄﾞﾗｲﾊﾞｰ ﾕﾆｯﾄ  ﾀﾞｲﾅﾐｯｸ ﾊﾞﾝﾄﾞ ﾀｲﾌﾟ 種類  ｲﾝﾅｰ ｲﾔｰ ﾀｲﾌﾟ ﾍｯﾄﾞｾｯﾄ 属性  携帯 電話 ﾍｯﾄﾞﾎﾝ 種類  密閉 型 HP ﾏｲｸ 有 伝送 種類  Bluetth 音声 ｱｼｽﾀﾝﾄ 機能 有  ｵｰﾃﾞｨｵ ﾃｸﾆｶ  audi − technica \n",
      "8 間違え て 購入 し て しまい まし た  17980 円 で 購入 し まし た  np  bu 10 情報 ﾒｰｶｰ 象印 ﾏﾎｰ ﾋﾞﾝ  ZOJIRUSHI  型番 B 428  6 B 梱包 ｻｲｽﾞ 22  2 x 21  8 x 13 cm 梱包 重量 0  98 ｷﾛｸﾞﾗﾑ 電池 使用 いいえ 電池 付属 いいえ ﾌﾞﾗﾝﾄﾞ 名 象印 ﾏﾎｰ ﾋﾞﾝ  ZOJIRUSHI  \n",
      "9 ･ 新品 未 使用 ･ iPhne Andrid 対応 ･ ｴｱｰﾎﾟｯｽﾞ で は ござい ませ ん の で ご 注意 下さい  ･ 7 枚 目 に 本体 箱 画像 載せ て おり ます  ･ 即日 発送 ･ 箱 無し 簡易 包装 で の 発送 に なり ます  ･ 箱 あり の 場合  300 円 に なり ます   IPX 6 は 完全 防水 で ｽﾏｰﾄ な ﾀｯﾁ   注  再生 が 中断 さ れる 場合 は  設定 で 自動 耳 検出 を ｵﾌ に し て ください   ｵｰﾌﾟﾝ ﾎﾟｰﾄ は 防水 ﾒｯｼｭ 素材 を 使用 し て い ます  これ に より  水 が 内部 に 入る の を 防ぎ  雨 や 発汗 を 心配 する こと なく 使用 でき ます  ｽﾎﾟｰﾂ や 運動 に 最適 な ｽﾎﾟｰﾂ ﾍｯﾄﾞｾｯﾄ です   注  充電 ｹｰｽ は 防水 で は あり ませ ん  指先 で 軽く ﾀｯﾌﾟ する と  いつ で も どこ で も 片手 で 操作 を でき ます    Hi  Fi 高 音質  最新 Bluetth 5  0  EDR 搭載   注  再生 が 中断 さ れる 場合 は  設定 で 自動 耳 検出 を ｵﾌ に し て ください   進化 し た ｸﾘｱ で ﾊﾟﾜﾌﾙ な ｻｳﾝﾄﾞ を 実現 し  ｽﾃﾚｵ 音楽 に 楽しめ ます  低音 から 高音 まで ﾊﾞﾗﾝｽ 良く 出 て い て  解像 度 も 高く ｷﾚ 感 が 気持ち 良い ｲﾔﾎﾝ  最 先端 Bluetth 5  0  EDR が 搭載 さ れ た ﾜｲﾔﾚｽ ｲﾔﾎﾝ で  信号 が 強く なり ます  さらに  ﾃﾞｰﾀ 伝送 速度 が 速く なっ た こと に より  遅延 や ﾉｲｽﾞ が 減り ます    抜群 の ﾌｨｯﾄ 感  ﾃｰﾊﾟｰﾄﾞ 型 の 内部 ｲﾔﾎﾝ は 耳 の 形 に ﾌｨｯﾄ し  耳 に しっかり と 固 定 し ます  換気 ｼｽﾃﾑ と 組み合わせる こと で  ｽﾄﾚｽ の ﾊﾞﾗﾝｽ を 取る の に 役立ち ます    両耳 分離 設計  ｺﾝﾊﾟｸﾄ  超 軽量  磁石 を 吸着 ﾃﾞｻﾞｲﾝ を 採用 し て  保存 し やすい ﾎﾟｰﾀﾌﾞﾙ 充電 ｹｰｽ の 中 に  外装 材 は 丈夫 な 絶縁 材 で 作ら れ  耳 に ぴったり と ﾌｨｯﾄ し 外れ にくい の で  安心 し て ｽﾎﾟｰﾂ など を 楽しめ ます  とても 軽量 な ため 耳 が 疲れ にくい です    自動 接続 と 自動 ﾎﾟｯﾌﾟ ｱｯﾌﾟ  初めて Bluetth を 接続 し た 後  ｲﾔﾎﾝ を 再び 充電 ｹｰｽ から 取り外す と  ｲﾔﾎﾝ は 自動 的 に ｵﾝ に なり  ﾍﾟｱﾘﾝｸﾞ を 始め ます  iPhne で 2 ｰ 3 秒 後 に  電話 の ﾎﾟｯﾌﾟ ｱｯﾌﾟ が 自動 的 に ｲﾔﾎﾝ の 接続 し ます  ﾜｲﾔﾚｽ ｲﾔﾎﾝ iphne Galaxy Xperia Xiami neplus realme OPPO huawei sny ﾚﾃﾞｨｰｽ \n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(df_description[0:10]):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "trainings = [TaggedDocument(words = data.split(),tags = [i]) for i,data in enumerate(df_description)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Doc2Vec(documents= trainings, dm = 1, size=300, window=8, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('doc2vec.model')\n",
    "#model = models.Doc2Vec.load('doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.infer_vector([\"宜しく\", \"どうぞ\", \"付属\", \"品\", \"は\", \"画像\", \"が\", \"全て\", \"です\",  \"問題\", \"なく\", \"動作\", \"確認\", \"済み\", \"です\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 451M\n",
      "drwxr-xr-x 2 root root 4.0K Oct 21 12:33 \u001b[0m\u001b[01;34m__pycache__\u001b[0m/\n",
      "-rw-r--r-- 1 root root  85M Oct 21 13:46 doc2vec.model\n",
      "-rw-r--r-- 1 root root 183M Oct 21 13:46 doc2vec.model.docvecs.vectors_docs.npy\n",
      "-rw-r--r-- 1 root root 183M Oct 21 13:46 doc2vec.model.docvecs.vectors_docs_norm.npy\n",
      "-rw-r--r-- 1 root root 1.7K Oct 21 12:15 model_lgb.py\n",
      "-rw-r--r-- 1 root root 3.9K Oct 21 12:31 model_nn.py\n",
      "-rw-r--r-- 1 root root 2.0K Oct 21 12:31 model_ridge.py\n",
      "-rw-r--r-- 1 root root 2.4K Oct 21 12:28 util.py\n"
     ]
    }
   ],
   "source": [
    "ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
