{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from operator import itemgetter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_log_error, r2_score\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import re, MeCab\n",
    "from glob import glob\n",
    "import mojimoji\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import np_utils\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "#環境変数,\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../auth/My First Project.json\"\n",
    "bucket_name = \"pj_horidasimono\"\n",
    "prefix=\"dataset/train/ElectricalAppliance\"\n",
    "#最大表示行数の指定（ここでは50行を指定）\n",
    "pd.set_option('display.max_rows', 200)\n",
    "#最大表示列数の指定（ここでは50列を指定）\n",
    "pd.set_option('display.max_columns', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def road_data_from_gcs(bucket_name, prefix):\n",
    "    client = storage.Client()\n",
    "    blobs = client.list_blobs(bucket_name, prefix=prefix)\n",
    "    df = pd.DataFrame()\n",
    "    for blob in blobs:\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        r = storage.Blob(blob.name, bucket)\n",
    "        content = r.download_as_string()\n",
    "        df = df.append(pd.read_json(content))\n",
    "        print(f\"read file {blob.name}...\")\n",
    "\n",
    "    df = df.drop_duplicates(subset=\"url\")\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "def make_wakati(sentence):\n",
    "    # MeCabで分かち書き\n",
    "    sentence = tagger.parse(sentence)\n",
    "    sentence = mojimoji.zen_to_han(sentence)\n",
    "    # 半角全角英数字除去\n",
    "    #sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
    "    # 記号もろもろ除去\n",
    "    sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—♬◉ᴗ͈ˬ●★☆⭐️⭕⚡⚠o①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮♡⭐︎〇◎◆♦▼◇△□(：〜～＋=)／*&^%$#@!~`)♪ᴖ◡ᴖ{}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕＼〈〉？、､。｡・,\\./『』【】｢｣「」→←○《》≪≫\\n\\u3000⭕]+', \"\", sentence)\n",
    "    # 絵文字除去\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    sentence = emoji_pattern.sub(r'', sentence)\n",
    "    # スペースで区切って形態素の配列へ\n",
    "    #wakati = sentence.split(\" \")\n",
    "    # 空の要素は削除\n",
    "    #wakati = list(filter((\"\").__ne__, wakati))\n",
    "    return sentence\n",
    "\n",
    "def title_torkenize(sentence):\n",
    "    sentence = mojimoji.zen_to_han(sentence)\n",
    "    sentence = re.sub(\"[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒♬◉ᴗ͈ˬ—●★☆⭐️⭕⚡⚠o①②③④⑤⑥⑦⑧⑨⑩⑪⑫⑬⑭⑮♡⭐︎〇◎◆♦▼◇△□(：〜～＋=)／*&^%$#@!~`)♪ᴖ◡ᴖ{}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕＼〈〉？、､。｡・,\\./『』【】｢｣「」→←○《》≪≫\\n\\u3000]\", \" \", sentence)\n",
    "    sentence = re.sub(\"[あ-ん]\", \" \", sentence)\n",
    "    sentence = re.sub(\"( |　)+\", \" \", sentence)\n",
    "    sentence = sentence.lower()\n",
    "    #〇〇様専用を除く\n",
    "    sentence = re.sub(\"[^ ]*専用\", \"\", sentence)\n",
    "    sentence = re.sub(\"[^ ]*様\", \"\", sentence)\n",
    "    #1文字のアルファベットを除く\n",
    "    sentence = re.sub(\" [a-z]{1}[^(a-z)]\", \" \", sentence)\n",
    "    # 絵文字除去\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    sentence = emoji_pattern.sub(r'', sentence)\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def preprocess(df):\n",
    "    df[\"price\"] = df[\"price\"].str.replace(r\"\\D\", \"\").astype(np.float)\n",
    "    \n",
    "    #列ズレを修正\n",
    "    pattern = re.compile(r\"^(?!.*(傷や汚れあり|全体的に状態が悪い|やや傷や汚れあり|未使用に近い|目立った傷や汚れなし|新品、未使用)).+$\")\n",
    "    invalid = df[\"status\"].str.match(pattern)\n",
    "\n",
    "    df.loc[invalid, \"description\"] = df.loc[invalid, \"description\"] + \"\\n\" + df.loc[invalid, \"status\"]\n",
    "    df.loc[invalid, \"status\"]      = df.loc[invalid, \"shipping\"]\n",
    "    df.loc[invalid, \"shipping\"]    = df.loc[invalid, \"method\"]\n",
    "    df.loc[invalid, \"method\"]      = df.loc[invalid, \"region\"]\n",
    "    df.loc[invalid, \"period\"]      = \"未定\"\n",
    "    \n",
    "    df[\"title\"] = df[\"title\"] + \" \" + df[\"sub_category_1\"] + \" \" + df[\"sub_category_2\"] + \" \" + df[\"brand\"]\n",
    "    #df[\"text\"]  = df[\"title\"] + \" \" + df[\"description\"]\n",
    "\n",
    "    df = df.drop(columns=[\"sub_category_1\", \"sub_category_2\", \"brand\"])\n",
    "    \n",
    "    status_dict = {'新品、未使用': \"best\",\n",
    "                   '未使用に近い': \"Very Good\",\n",
    "                   '目立った傷や汚れなし': \"good\",\n",
    "                   '傷や汚れあり': \"Poor\",\n",
    "                   'やや傷や汚れあり': \"very poor\",\n",
    "                   '全体的に状態が悪い': \"worst\"\n",
    "                  }\n",
    "    \n",
    "    #配送負担をラベルエンコーディング\n",
    "    shipping_dict = {'送料込み(出品者負担)': 0, '着払い(購入者負担)': 1}\n",
    "\n",
    "    df[\"status\"] = df[\"status\"].map(status_dict)\n",
    "    df[\"shipping\"] = df[\"shipping\"].map(shipping_dict)\n",
    "    \n",
    "    #トークナイズ\n",
    "    df[\"title\"] = df[\"title\"].apply(title_torkenize)\n",
    "    df[\"description\"] = df[\"description\"].apply(make_wakati)\n",
    "    \n",
    "    #不要列削除\n",
    "    df = df.drop(columns=[\"url\", \"seller\", \"rating\", \"method\", \"region\", \"period\", \"recent_comment\", \"timestamp\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/user/project/pj_horidasimono/code-analysis\n"
     ]
    }
   ],
   "source": [
    "cd code-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_ridge import ModelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ = road_data_from_gcs(bucket_name, prefix)\n",
    "df_ = pd.read_pickle(\"dataset.pickle\")\n",
    "#df_.to_pickle(\"dataset.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess] done in 158 s\n",
      "[fit and predict] done in 879 s\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "def on_field(f: str, *vec):\n",
    "    return make_pipeline(FunctionTransformer(itemgetter(f), validate=False), *vec)\n",
    "\n",
    "def to_records(df: pd.DataFrame):\n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "vectorizer = make_union(\n",
    "            on_field(\"title\", Tfidf(max_features=100000, token_pattern=\"\\w+\")),\n",
    "            on_field(\"description\", Tfidf(max_features=500000, token_pattern=\"\\w+\", ngram_range=(1, 2))),\n",
    "            on_field(['shipping', 'status'],\n",
    "                 FunctionTransformer(to_records, validate=False), DictVectorizer()))\n",
    "\n",
    "with timer(\"preprocess\"):\n",
    "    df = preprocess(df_.copy())\n",
    "    X = df.drop(columns=\"price\")\n",
    "    y = df[\"price\"]\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    X_train = vectorizer.fit_transform(X_train).astype(np.float32)\n",
    "    X_valid = vectorizer.transform(X_valid).astype(np.float32)\n",
    "    Xb_train, Xb_valid = [x.astype(np.bool).astype(np.float32) for x in [X_train, X_valid]]\n",
    "    \n",
    "    xs = [[Xb_train, Xb_valid], [X_train, X_valid]] * 2\n",
    "\n",
    "with timer(\"fit and predict\"):\n",
    "    preds = []\n",
    "    for X_train, X_valid in xs:\n",
    "        model = ModelRidge()\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = np.expm1(model.predict(X_valid))\n",
    "        preds.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3921.00073941, 10722.54178525,  3015.72987121,  6307.71814086,\n",
       "         5837.41511324,  4124.91457758,  3318.73857373, 10388.81354219,\n",
       "         9276.01570235,  4403.37482128],\n",
       "       [ 5033.77274055,  8672.69665585,  3291.10274213,  7334.94270278,\n",
       "         6168.25854082,  4247.94608789,  4581.66158334,  7324.05772586,\n",
       "         9005.50807571,  4207.85018395],\n",
       "       [ 3925.23032733, 10715.00034578,  3016.61798409,  6306.63922998,\n",
       "         5834.43025584,  4127.43910608,  3317.39577228, 10391.9404367 ,\n",
       "         9283.17776722,  4396.11391128],\n",
       "       [ 5033.77423575,  8672.70156269,  3291.10094743,  7334.93930372,\n",
       "         6168.25372722,  4247.94592174,  4581.66296965,  7324.04847944,\n",
       "         9005.50674726,  4207.85583122]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(preds)[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584205749403832\n",
      "49.22214812364832\n",
      "0.4560177175726792\n",
      "36.77099050851783\n",
      "0.5842012810989269\n",
      "49.22075733874581\n",
      "0.4560177162274387\n",
      "36.77099165179877\n",
      "0.48854135389353986\n"
     ]
    }
   ],
   "source": [
    "for pred in np.array(preds):\n",
    "    print(np.sqrt(mean_squared_log_error(y_valid, pred)))\n",
    "    print(np.mean(np.abs((y_valid - pred) / y_valid)) * 100)\n",
    "\n",
    "print(np.sqrt(mean_squared_log_error(y_valid, np.mean(np.array(preds), axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "va_pred = np.expm1(model.predict(X_valid))\n",
    "score = np.sqrt(mean_squared_log_error(y_valid, va_pred))\n",
    "mape = np.mean(np.abs((y_valid - va_pred) / y_valid)) * 100\n",
    "print(\"mape: {:.3f}%\".format(mape))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "3 4\n"
     ]
    }
   ],
   "source": [
    "a = [[1, 2], [3, 4]]\n",
    "for i, j in a:\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Doc2Vec(documents= trainings, dm = 1, size=300, window=8, min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save('doc2vec.model')\n",
    "#model = models.Doc2Vec.load('doc2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.infer_vector([\"宜しく\", \"どうぞ\", \"付属\", \"品\", \"は\", \"画像\", \"が\", \"全て\", \"です\",  \"問題\", \"なく\", \"動作\", \"確認\", \"済み\", \"です\"]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "vectorizer = Vectorizer().tfidf_vectorizer(title_feat=100000, description_feat=500000)\n",
    "\n",
    "with timer(\"preprocess\"):\n",
    "    df = preprocess(df_.copy())\n",
    "    X = df.drop(columns=\"price\")\n",
    "    y = df[\"price\"]\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    X_train = vectorizer.fit_transform(X_train).astype(np.float32)\n",
    "    X_valid = vectorizer.transform(X_valid).astype(np.float32)\n",
    "    Xb_train, Xb_valid = [x.astype(np.bool).astype(np.float32) for x in [X_train, X_valid]]\n",
    "    \n",
    "    xs = [[Xb_train, Xb_valid], [X_train, X_valid]] * 2\n",
    "\n",
    "with timer(\"fit and predict\"):\n",
    "    preds = []\n",
    "    for X_train, X_valid in xs:\n",
    "        model = ModelRidge()\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = np.expm1(model.predict(X_valid))\n",
    "        preds.append(pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
